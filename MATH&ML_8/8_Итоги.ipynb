{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "✍ Итак, мы рассмотрели алгоритм дерева решений от его корней до самых листьев.\n",
        "\n",
        "Мы изучили алгоритм CART для построения деревьев и даже самостоятельно реализовали его на Python. Теперь вы понимаете, как обучаются деревья и как они прогнозируют целевую переменную для новых данных как в задачах классификации, так и в задачах регрессии.\n",
        "\n",
        "Также мы научились определять значимость каждого из признаков, представленных для обучения дерева.\n",
        "\n",
        "Резюмируя, приведём основные **преимущества и недостатки деревьев:**\n",
        "\n",
        "$+:$\n",
        "\n",
        "+ Алгоритм способен решать как задачи классификации, так и задачи регрессии.\n",
        "\n",
        "+ Простой для понимания и интерпретируемый алгоритм, понятный даже людям, не знакомым с машинным обучением. Можно визуализировать в виде графа.\n",
        "\n",
        "+ Не требуется нормализация/стандартизация данных.\n",
        "\n",
        "+ В общем случае деревья могут обрабатывать как числовые, так и категориальные данные. Однако в sklearn категориальные признаки, представленные в нечисловом формате, не поддерживаются.\n",
        "\n",
        "+ Дерево способно решать задачи с несколькими выходами, то есть когда целевая переменная имеет размерность больше 1.\n",
        "\n",
        "+ На основе значимости признаков можно производить их отбор. Однако коэффициенты значимости напрямую зависят от сложности дерева.\n",
        "\n",
        "$-:$\n",
        "\n",
        "+ В силу дискретной топологической структуры дерево не дифференцируется по параметрам — стандартные алгоритмы поиска параметров, такие как градиентный спуск, не работают. Приходится использовать полный перебор, что значительно увеличивает время построения модели, особенно для выборок больших размеров.\n",
        "\n",
        "+ Чрезвычайно сильная склонность к переобучению. Необходим подбор внешних параметров, таких как максимальная глубина, минимальное количество объектов для разделения вершины и других.\n",
        "\n",
        "+ Деревья решений могут быть нестабильными: небольшое изменение в данных способно заметно повлиять на структуру дерева.\n",
        "\n",
        "+ Функция дерева является кусочно-постоянной, поэтому деревья не могут использоваться для экстраполяции (предсказания целевой переменной за границами обучающей выборки).\n",
        "\n",
        "+ Поиск оптимального по структуре дерева решений является NP-полной задачей. Следовательно, практическая реализация дерева решений основана на эвристиках, таких как жадный алгоритм. Он, в свою очередь, не гарантирует того, что дерево будет самым оптимальным.\n",
        "\n",
        "+ Дерево может плохо справляться с несбалансированными данными.\n",
        "\n",
        "\n",
        "Как вы уже знаете, от многих из представленных недостатков деревьев можно избавиться, если объединить несколько деревьев в ансамбли. В следующем модуле мы как раз поговорим о математической формализации ансамблевых методов."
      ],
      "metadata": {
        "id": "HHihWXtiSCiX"
      }
    }
  ]
}